<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  VƒÉn-Th·ªëng  Hu·ª≥nh


  | publication

</title>

<!--  <meta name="description" content="Selected publications, multimodal emotion, engagement prediction, emotiw 2019, openeds 2019, eye segmentation, pain estimation" />
 -->

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<!-- <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://github.com/jwarby/jekyll-pygments-themes/blob/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®‚Äçüéì</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publication/">


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-151503814-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-151503814-1');
</script>




    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>publication | blank</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="publication" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://vthuynh.net/publication/" />
<meta property="og:url" content="https://vthuynh.net/publication/" />
<meta property="og:site_name" content="blank" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="publication" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"publication","url":"https://vthuynh.net/publication/"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://vthuynh.net/">
       <span class="font-weight-bold">VƒÉn-Th·ªëng</span>   Hu·ª≥nh
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publication/">
                publication
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publication</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    <h2 class="bibliography">2023</h2>
<ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Patt. Reco. Lett.</abbr>
    
  
  </div>

  <div id="HUYNH2023245" class="col-sm-9">
    
      <div class="title">Prediction of evoked expression from videos with temporal position fusion</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Hyung-Jeong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Pattern Recognition Letters</em>
        
            172
        
        
      
      <!--
        pp. 245-251
      -->
      <!--
        2023
      -->
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="/papers/evoked_prl23" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/th2l/EvokedExpr-TPF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><i> This paper introduces an approach for estimating evoked categories expression from videos with the temporal position fusion. Pre-trained models on large-scale datasets in computer vision and audio signals were used to extract the deep representation for timestamps in the video. A temporal convolution network, rather than an RNN-like architecture, was applied to explore temporal relationships due to its advantage in memory consumption and parallelism. Furthermore, to address the noise labels, the temporal position was fused with the deep learned feature to ensure the network differentiates the time steps when noise labels were removed from the training set. This technique helps the system gain a considerable improvement compared to other methods. We conducted experiments on EEV, a large-scale dataset for evoked expression from videos, and achieved a score of 0.054 in terms of Pearson correlation coefficient as a state-of-the-art result. Further experiments on a sub set of LIRIS-ACCEDE dataset - MediaEval 2018 benchmark, also demonstrated the effectiveness of our approach. </i> </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">HUYNH2023245</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prediction of evoked expression from videos with temporal position fusion}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pattern Recognition Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{172}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{245-251}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0167-8655}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>
  
  
  
  
  
  
  
  
  
  
  
    <h2 class="bibliography">2021</h2>
<ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE Multimedia</abbr>
    
  
  </div>

  <div id="vthuynhIEEEM" class="col-sm-9">
    
      <div class="title">End-to-End Learning for Multimodal Emotion Recognition in Video With Adaptive Loss</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Hyung-Jeong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE MultiMedia</em>
        
            28
        
        
            (2)
        
      
      <!--
        pp. 59-66
      -->
      <!--
        2021
      -->
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="/papers/e2e_mm21" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/th2l/E2EMAL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><i> This work presents an approach for emotion recognition in video through the interaction of visual, audio, and language information in an end-to-end learning manner with three key points: 1) lightweight feature extractor, 2) attention strategy, and 3) adaptive loss. We proposed a lightweight deep architecture with approximately 1 MB, which for the most crucial part, accounts for feature extraction, in the emotion recognition systems. The relationship in regard to the time dimension of features is explored with temporal convolutional network instead of RNNs-based architecture to leverage the parallelism and avoid the challenge of vanishing gradient. The attention strategy is employed to adjust the knowledge of temporal networks based on the time dimension and learning of each modality‚Äôs contribution to the final results. The interaction between the modalities is also investigated when training with adaptive objective function, which adjusts the network‚Äôs gradient. The experimental results obtained on a large-scale dataset for emotion recognition on Koreans demonstrate the superiority of our method when employing attention mechanism and adaptive loss during training. </i> </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vthuynhIEEEM</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE MultiMedia}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{End-to-End Learning for Multimodal Emotion Recognition in Video With Adaptive Loss}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{28}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{59-66}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>
  
  
  
  
  
  
    <h2 class="bibliography">2020</h2>
<ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">FG 2020</abbr>
    
  
  </div>

  <div id="yang2020multimodality" class="col-sm-9">
    
      <div class="title">Multimodality Pain and related Behaviors Recognition based on Attention Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Hyung-Jeong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</em>
      
      <!--
        pp. 814‚Äì818
      -->
      <!--
        2020
      -->
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="/papers/emopain_fg20" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><i> Our work aimed to study facial data as well as movement data for recognition of pain and related behaviors in the context of everyday physical activities, which was provided as three tasks in EmoPain 2020 challenge. We explored deep visual representation and geometric features, which included head pose, facial landmarks, and action units in facial data with a combination of fully connected layers for estimating pain from facial data. In tasks with movement data, we employed long short-term memory layers to learn temporal information in each segment of 180 frames. We examined attention mechanism to investigate the relationship and gather data from multiple sources together. Experiments on EmoPain dataset showed that our methods significantly outperformed baseline results on pain recognition tasks. </i> </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yang2020multimodality</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multimodality Pain and related Behaviors Recognition based on Attention Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{814--818}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE Access</abbr>
    
  
  </div>

  <div id="vthuynh2020semantic" class="col-sm-9">
    
      <div class="title">Semantic Segmentation of the Eye With a Lightweight Deep Network and Shape Correction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Hyung-Jeong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Access</em>
        
            8
        
        
      
      <!--
        pp. 131967‚Äì131974
      -->
      <!--
        2020
      -->
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/9143078" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/th2l/Eye_VR_Segmentation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><i> This paper presents a method to address the multi-class eye segmentation problem which is an essential step for gaze tracking or applying a biometric system in the virtual reality environment. Our system can run on the resource-constrained environments, such as mobile, embedded devices for real-time inference, while still ensuring the accuracy. To achieve those ends, we deployed the system with three major stages: obtain a grayscale image from the input, divide the image into three distinct eye regions with a deep network, and refine the results with image processing techniques. The deep network is built upon an encoder-decoder scheme with depthwise separation convolution for the low-resource systems. Image processing is accomplished based on the geometric properties of the eye to remove incorrect regions as well as to correct the shape of the eye. The experiments were conducted using OpenEDS, a large dataset of eye images captured with a head-mounted display with two synchronized eye-facing cameras. We achieved a mean intersection over union (mIoU) of 94.91% with a model of size 0.4 megabytes and 16.56 seconds to iterate over the test set of 1,440 images. </i> </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vthuynh2020semantic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantic Segmentation of the Eye With a Lightweight Deep Network and Shape Correction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Access}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{131967--131974}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>
  
  
  
  
  
  
    <h2 class="bibliography">2019</h2>
<ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCVW 2019</abbr>
    
  
  </div>

  <div id="vthuynh2019eye" class="col-sm-9">
    
      <div class="title">Eye semantic segmentation with a lightweight model</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Yang, Hyung-Jeong
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>
      
      <!--
        pp. 3694‚Äì3697
      -->
      <!--
        2019
      -->
      </div>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/9022251" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
      <a href="https://github.com/th2l/Eye_VR_Segmentation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vthuynh2019eye</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Eye semantic segmentation with a lightweight model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Kim, Soo-Hyung and Lee, Guee-Sang and Yang, Hyung-Jeong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3694--3697}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICMI 2019</abbr>
    
  
  </div>

  <div id="thong2019engagement" class="col-sm-9">
    
      <div class="title">Engagement Intensity Prediction with Facial Behavior Features</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Yang, Hyung-Jeong
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2019 International Conference on Multimodal Interaction</em>
      
      <!--
        pp. 567‚Äì571
      -->
      <!--
        2019
      -->
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="/papers/eg_emotiw19" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
      
      <a href="https://vthuynh.net/assets/pdf/ICMI19_EG_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p><i> This paper describes an approach for the engagement prediction task, a sub-challenge of the 7th Emotion Recognition in the Wild Challenge (EmotiW 2019). Our method involves three fundamental steps: feature extraction, regression and model ensemble. In the first step, an input video is divided into multiple overlapped segments (instances) and the features extracted for each instance. The combinations of Long short-term memory (LSTM) and Fully connected layers deployed to capture the temporal information and regress the engagement intensity for the features in previous step. In the last step, we performed fusions to achieve better performance. Finally, our approach achieved a mean square error of 0.0597, which is 4.63% lower than the best results last year. </i> </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">thong2019engagement</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Engagement Intensity Prediction with Facial Behavior Features}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Kim, Soo-Hyung and Lee, Guee-Sang and Yang, Hyung-Jeong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{567--571}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICMLSC 2019</abbr>
    
  
  </div>

  <div id="van2019emotion" class="col-sm-9">
    
      <div class="title">Emotion recognition by integrating eye movement analysis and facial expression model</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Hyung-Jeong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Guee-Sang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Na, In-Seop
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing</em>
      
      <!--
        pp. 166‚Äì169
      -->
      <!--
        2019
      -->
      </div>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://dl.acm.org/doi/abs/10.1145/3310986.3311001" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">van2019emotion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Emotion recognition by integrating eye movement analysis and facial expression model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung and Na, In-Seop}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 3rd International Conference on Machine Learning and Soft Computing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{166--169}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>
  
  
  
  
  
  
    <h2 class="bibliography">2018</h2>
<ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICMLSC 2018</abbr>
    
  
  </div>

  <div id="huynh2018learning" class="col-sm-9">
    
      <div class="title">Learning to detect tables in document images using line and text information</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Huynh, Van-Thong</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=ha11OwIAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Nguyen-An, Khuong</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://lbktrinh.github.io/" target="_blank" rel="noopener noreferrer">Khanh, Trinh Le Ba</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Hyung-Jeong,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sites.google.com/site/trtanh1988/" target="_blank" rel="noopener noreferrer">Tran, Tuan Anh</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://prlabjnu.github.io/shkim" target="_blank" rel="noopener noreferrer">Kim, Soo-Hyung</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2nd International Conference on Machine Learning and Soft Computing</em>
      
      <!--
        pp. 151‚Äì155
      -->
      <!--
        2018
      -->
      </div>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://dl.acm.org/doi/abs/10.1145/3184066.3184091" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huynh2018learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to detect tables in document images using line and text information}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huynh, Van-Thong and Nguyen-An, Khuong and Khanh, Trinh Le Ba and Yang, Hyung-Jeong and Tran, Tuan Anh and Kim, Soo-Hyung}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2nd International Conference on Machine Learning and Soft Computing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{151--155}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>
  
  
  
  
  
  
  
  
  
  
  
  


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2024 VƒÉn-Th·ªëng  Hu·ª≥nh.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-151503814-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-151503814-1');
</script>





</html>
