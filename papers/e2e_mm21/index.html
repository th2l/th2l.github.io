<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  VƒÉn-Th·ªëng  Hu·ª≥nh


  | End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss

</title>

<!---->
<meta name="description" content="">
<!--  -->

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<!-- <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://github.com/jwarby/jekyll-pygments-themes/blob/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®‚Äçüéì</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/papers/e2e_mm21/">


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-151503814-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-151503814-1');
</script>




    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss | blank</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="IEEE MultiMedia, Vol. 28 (2), pp. 59-66, April-June 2021" />
<meta property="og:description" content="IEEE MultiMedia, Vol. 28 (2), pp. 59-66, April-June 2021" />
<link rel="canonical" href="https://vthuynh.net/papers/e2e_mm21/" />
<meta property="og:url" content="https://vthuynh.net/papers/e2e_mm21/" />
<meta property="og:site_name" content="blank" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-04T01:34:39+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-07-04T01:34:39+00:00","datePublished":"2024-07-04T01:34:39+00:00","description":"IEEE MultiMedia, Vol. 28 (2), pp. 59-66, April-June 2021","headline":"End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss","mainEntityOfPage":{"@type":"WebPage","@id":"https://vthuynh.net/papers/e2e_mm21/"},"url":"https://vthuynh.net/papers/e2e_mm21/"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://vthuynh.net/">
       <span class="font-weight-bold">VƒÉn-Th·ªëng</span>   Hu·ª≥nh
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publication/">
                publication
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h2 class="post-title" style="text-align:center;">End-to-End Learning for Multimodal Emotion Recognition in Video with Adaptive Loss</h2>
    <p class="post-description" style="text-align:center;">IEEE MultiMedia, Vol. 28 (2), pp. 59-66, April-June 2021</p>
  </header>

  <article class="post-content">
    <h3 id="abstract">Abstract:</h3>

<p>This work presents an approach for emotion recognition in video through the interaction of visual, audio, and language information in an end-to-end learning manner with three key points: <strong>1)</strong> lightweight feature extractor, <strong>2)</strong> attention strategy, and <strong>3)</strong> adaptive loss. We proposed a lightweight deep architecture with approximately 1 MB, which for the most crucial part, accounts for feature extraction, in the emotion recognition systems. The relationship in regard to the time dimension of features is explored with temporal convolutional network instead of RNNs-based architecture to leverage the parallelism and avoid the challenge of vanishing gradient. The attention strategy is employed to adjust the knowledge of temporal networks based on the time dimension and learning of each modality‚Äôs contribution to the final results. The interaction between the modalities is also investigated when training with adaptive objective function, which adjusts the network‚Äôs gradient. The experimental results obtained on a large-scale dataset for emotion recognition on Koreans demonstrate the superiority of our method when employing attention mechanism and adaptive loss during training.</p>

<table>
  <tbody>
    <tr>
      <td>
<a href="https://ieeexplore.ieee.org/document/9431699" target="_blank" rel="noopener noreferrer">Full paper</a> | <a href="https://github.com/th2l/E2EMAL" target="_blank" rel="noopener noreferrer">Code</a>
</td>
    </tr>
  </tbody>
</table>

<p><img class="rounded mx-auto d-block" src="/assets/img/papers/e2e_mm_overview.png" alt="Method overview" width="100%" height="auto"></p>

<p><img class="rounded mx-auto d-block" src="/assets/img/papers/e2e_mm_lfe.png" alt="LFE block" width="100%" height="auto"></p>

<h4 id="evaluation">Evaluation</h4>

<ol>
  <li>
    <p><strong>CMU-MOSEI</strong> (CMU Multimodal Opinion Sentiment and Emotion Intensity)</p>

    <p>F1 score and Weighted accuracy (wA):</p>

\[wA = \frac{TP(N/P) + TN}{2N},\]

    <p>where TP and TN are true positive, false negative, P and N are the toal number of positive, negative.</p>

    <figure class="figure">
     <img class="rounded mx-auto d-block" src="/assets/img/papers/e2e_mm_cmu-mosei.png" alt="CMU-MOSEI comparison" width="100%" height="auto">
     <figcaption class="figure-caption">Evaluation results on CMU-MOSEI compared to prior works.</figcaption>
 </figure>
  </li>
  <li>
    <p><strong>MERC2020</strong> (<a href="https://sites.google.com/view/merc2020/" target="_blank" rel="noopener noreferrer">Korean Emotion Recognition Dataset</a>)</p>

    <figure class="figure">
     <img class="rounded mx-auto d-block" src="/assets/img/papers/e2e_mm_merc_val.png" alt="CMU-MOSEI comparison" width="60%" height="auto">
     <figcaption class="figure-caption" style="text-align:center;">Evaluation results on MERC2020 validation set.</figcaption>
 </figure>
  </li>
</ol>

<h4 id="citation">Citation</h4>

<p>V. T. Huynh, H. -J. Yang, G. -S. Lee and S. -H. Kim, ‚ÄúEnd-to-End Learning for Multimodal Emotion Recognition in Video With Adaptive Loss,‚Äù in <em>IEEE MultiMedia</em>, vol. 28, no. 2, pp. 59-66, 1 April-June 2021, doi: 10.1109/MMUL.2021.3080305.</p>

<p><strong>BibTeX</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@ARTICLE{9431699,
  author={Huynh, Van Thong and Yang, Hyung-Jeong and Lee, Guee-Sang and Kim, Soo-Hyung},
  journal={IEEE MultiMedia},
  title={End-to-End Learning for Multimodal Emotion Recognition in Video With Adaptive Loss},
  year={2021},
  volume={28},
  number={2},
  pages={59-66},
  doi={10.1109/MMUL.2021.3080305}}
</code></pre></div></div>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2024 VƒÉn-Th·ªëng  Hu·ª≥nh.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-151503814-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-151503814-1');
</script>





</html>
